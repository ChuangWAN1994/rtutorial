---
title: "R tutorial - regression"
author: "Wooyong Lee"
header-includes:
- \usepackage{amsfonts,amssymb,amsmath}
- \usepackage{graphicx}
- \usepackage{setspace}
- \usepackage{cleveref}
output:
  html_document:
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r header, echo=FALSE, message=FALSE, warning=FALSE}
Sys.setenv(lang="EN")
```


# linear regression

## lm function

Let's use this dataset as an example.

```{r lm}
library(datasets)
head(mtcars) # this prints the first 6 observations of the dataset
```

If we want to regress `mpg` on a constant and `wt`, we use the `lm` function.

```{r lm2}
regressionFit = lm(mpg ~ wt, data = mtcars)
```

We saved the return value of `lm` to `regressionFit`. It is a complicated object:

```{r lm3}
str(regressionFit)
```

But we see familiar names such as `coefficients`, `residuals`, and `fitted.values`. We can access these by the following.

```{r lm3-2}
regressionFit$coefficients
regressionFit$residuals
regressionFit$fitted.values
```

To see the usual results that we get from other languages, type these:

```{r lm4}
regressionFit
print(regressionFit)
summary(regressionFit)
```

To run a regression without a constant:

```{r lm5}
regFitWithoutConst = lm(mpg ~ -1 + wt, data=mtcars)
summary(regFitWithoutConst)
```

We can also add other regressors.

```{r lm6}
regressionFit = lm(mpg ~ wt + cyl + disp, data=mtcars)
summary(regressionFit)
```

As we have seen in the `ggplot2` tutorial, the variable `cyl` has only three values: `4`, `6`, `8`.

We may want to treat `cyl` as a categorical variable.

To do this we regress `mpg` on `factor(cyl)`. The `factor` function transforms numerical values into categorical values.

```{r lm7}
regressionFit = lm(mpg ~ wt + factor(cyl) + disp, data=mtcars)
summary(regressionFit)
```

If you want to use `wt^2` as a regressor, there are two ways to do it. One way is to create another column in the `data.frame`.

```{r lm8}
mtcars$wt2 = mtcars$wt^2 # the dataframe creates the column wt2 and assign the values.
head(mtcars)
summary(lm(mpg ~ wt + wt2, data=mtcars))
```

Another way that does not involve creating a column is the following.

```{r lm9}
summary(lm(mpg ~ wt + I(wt^2), data=mtcars))
```

The sum works similarly: e.g. `I(cyl+disp)`.

What is the function `I()`? The answer is related to what we discuss in the next section.

## the formula

We have been omitting the label for the first argument. The first argument of the `lm` function is `formula`:

```{r formula}
lm(formula = mpg ~ wt + disp, data=mtcars)
```

`formula` is a special object that contains an "expression". Note that we don't need to specify `mpg ~ wt + disp` as string, in which case we need to write `"mpg ~ wt + disp"` (of course you can also supply a string to `formula`: it will automatically be converted to a formula object).

In this "expression", the operators like `~` and `+` work differently from the usual way. For example, `+` in the formula is not an arithmetic operator but an operator that says we have multiple regressors.

The function `I` specifies that we want to read operators like `+` and `^` inside `I` as arithmetic operators. So the operator `^` in `I(wt^2)` is interpreted as a power operator. Similarly, `I(cyl+disp)` interprets `+` as an arithmetic operator and not the operator that says we have two regressors `cyl` and `disp`.

# heteroskedasticity

The `lm` function computes standard errors under homoskedasticity. To compute heteroskedasticity-robust standard errors, we need the `sandwich` and the `lmtest` package. 

```{r sandwich}
library(sandwich)
library(lmtest)
```

We use the following regression as an example.

```{r}
lfit = lm(formula = mpg ~ wt + disp, data=mtcars)
summary(lfit)
```

We use the `vcovHC` function in the `sandwich` package to compute the robust standard errors:

```{r}
vcHC = vcovHC(lfit, type = "HC0")
```

Note that we use the result of the `lm` function as the first argument of `vcovHC`. 

The above command computes the variance-covariance matrix by the estimator

$$
  vcHC = (X'X)^{-1}X'\Omega X(X'X)^{-1}
$$

where $X$ is the matrix of regressors and 
$$
  \Omega = \text{diag}(u_1^2, \ldots, u_N^2)
$$
where $u_i$ is the residual for individual $i$.

We can also write `HC1`, `HC2` or `HC3` instead of `HC0` in the `type` argument. These are all finite-sample corrections to the above estimator.

Now, to generate the analog of `summary(lfit)`, we use the `coeftest` function in the `lmtest` package:

```{r}
coeftest(lfit, vcov. = vcHC)
```

Compare the above result with the result of the `lm` function. We can see that the standard errors are corrected in the above. 

```{r}
summary(lfit)
```

# clustered errors

We can also compute clustered standard errors using `sandwich` package. However, to do so, we need to tell `vcovHC` that which variables represent the group and the time indices. We need the `plm` package for this.

```{r}
library(plm)
```

The `plm` function in the `plm` package performs several kinds of panel data regressions. For now, we use `plm` to do exactly what `lm` does (i.e. OLS), except that we also store information on the group and the time indices. 

We use the following dataset as an example.

```{r}
data("Grunfeld", package = "plm")
head(Grunfeld)
```

Consider the following OLS.

```{r}
pfitl = lm(inv ~ value + capital, data=Grunfeld)
summary(pfitl)
```

The following code produces exactly the same result:

```{r}
pfit = plm(inv ~ value + capital, model="pooling", index="firm", data=Grunfeld)
summary(pfit)
```

The option `model="pooling"` makes `plm` to run OLS. The option `index=firm` specifies that the observations are grouped according to the `firm` variable.

Now we create the clustered standard errors:

```{r}
vcHCcluster = vcovHC(pfit, type = "HC0", cluster = "group")
```

Note that the value for the argument `cluster`, which is `"group"`, is not a variable name in the dataset `Grunfeld`. The option `cluster = "group"` tells that we use the group index saved in `pfit` as the cluster.

The above command computes the estimator

$$
  vcHCcluster = \left(\sum_{k=1}^K X_k'X_k\right)^{-1} \sum_{k=1}^K X_k'U_kU_k'X_k \left(\sum_{k=1}^K X_k'X_k\right)^{-1}
$$

where $X_k$ is the matrix of regressors for the $k$th cluster and $U_k$ is the residual vector for the $k$th cluster.

To produce the regression result with clustered standard errors, we again use the `coeftest` function.

```{r}
coeftest(pfit, vcov. = vcHCcluster)
```

# probit and logit

We use `glm` to run probit and logit regressions. The usage is very similar to `lm`.

```{r glm}
# recall:
head(mtcars)

# let's run probit with some random formula. 
probitFit = glm(am ~ mpg + disp, family = binomial(link="probit"), data = mtcars)
probitFit
summary(probitFit)

# let's run logit.
logitFit = glm(am ~ mpg + disp, family = binomial(link="logit"), data = mtcars)
logitFit
summary(logitFit)

```









